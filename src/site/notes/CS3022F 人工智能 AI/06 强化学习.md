---
{"dg-publish":true,"permalink":"/CS3022F 人工智能 AI/06 强化学习/","dgPassFrontmatter":true,"noteIcon":""}
---


## 01 强化学习定义：马尔科夫决策过程

在智能主体与环境的交互中，学习能最大化收益的行动模式：
![Pasted image 20250529201321.png|375](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250529201321.png)

### 离散马尔可夫过程 Discrete Markov Process

#### 基本概念

随机过程：是一列随时间变化的随机变量;
- 当时间是离散量时，一个随机过程可以表示为 $\{X_t\}_{t=0,1,2,\cdotp\cdotp\cdotp}$, 其中每个 $X_t$ 都是一个随机变量，这被称为离散随机过程

马尔可夫链（Markov Chain）：满足马尔可夫性（Markov Property）的离散随机过程，也被称为离散马尔科夫过程
- 𝒕+𝟏时刻状态仅与𝒕时刻状态相关
- 二阶：𝒕 +𝟏时刻状态与𝒕和𝒕−𝟏时刻状态相关

马尔可夫奖励过程（Markov Reward Process）：引入奖励
- 奖励函数 $R{:}S\times S\mapsto\mathbb{R}$, 其中 $R(S_t,S_{t+1})$ 描述了从第 $t$ 步状态转移到第 $t+1$ 步状态所获得奖励
- 在一个序列决策过程中，不同状态之间的转移产生了一系列的奖励 $(R_1,R_2,\cdotp\cdotp\cdotp)$, 其中 $R_{t+1}$ 为 $R(S_t,S_{t+1})$ 的简便记法
- 为了比较不同的奖励序列，定义反馈 (return), 用来反映累加奖励：
	$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots $$
	其中衰退系数 (decay factor) $\gamma\in[0,1]$


马尔可夫决策过程（Markov Decision Process）：引入动作
- 定义智能主体能够采取的动作集合为 $A$
	可以是无限的
- 由于不同的动作对环境造成的影响不同，因此状态转移概率定义为 $Pr(S_{t+1}|S_t,a_t)$，其中 $a_t \in A$ 为第 $t$ 步采取的动作
	可以是随机概率性的转移
- 奖励可能受动作的影响，因此修改奖励函数为 $R(S_t,a_t,S_{t+1})$

#### 例子

![Pasted image 20250529204300.png|475](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250529204300.png)

使用离散马尔可夫决策过程描述机器人移动问题：
- 随机变量序列 $\{S_t\}_{t=0,1,2,\cdots}$: $S_t$ 表示机器人第 $t$ 步所在位置（即状态），每个随机变量 $S_t$ 的取值范围为 $S=\{s_1,s_2,\cdots,s_9,s_d\}$
- 动作集合: $A=\{\text{上},\text{右}\}$
- 状态转移概率 $Pr(S_{t+1}|S_t,a_t)$: 满足马尔可夫性，其中 $a_t\in A$。状态转移
- 奖励函数：$R(S_t,a_t,S_{t+1})$ 
- 衰退系数：$\gamma\in[0,1]$

综合以上信息，可通过 $MDP=\{S,A,Pr,R,\gamma\}$ 来刻画马尔科夫决策过程
- 马尔可夫决策过程 $MDP=\{S,A,Pr,R,\gamma\}$ 是刻画强化学习中环境的标准形式
- 马尔可夫决策过程可用如下序列来表示：
![Pasted image 20250529204424.png|350](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250529204424.png)
- 马尔科夫过程中产生的状态序列称为轨迹 (trajectory), 可如下表示
	$$(S_0,a_0,R_1,S_1,a_1,R_2,\cdots,S_T)$$
- 轨迹长度可以是无限的，也可以有终止状态 $S_{T}$。有终止状态的问题叫做分段的 (episodic), 否则叫做持续的 (continuing)
	- 分段问题中，一个从初始状态到终止状态的完整轨迹称为一个片段 (episode)

#### 策略学习

智能主体如何与环境交互而完成任务？需要进行策略学习
- 已知的：S A R $\gamma$ 
- 不一定已知的：Pr
- 观察到的：$(S_0,a_0,R_1,S_1,a_1,R_2,...,S_T)$

策略函数：
- 策略函数 $\pi{:}S\times A\mapsto[0,1]$, 其中 $\pi(\mathfrak{s},a)$ 的值表示在状态 s 下采取动作 $a$ 的概率
- 策略函数的输出可以是确定的，即给定 s 情况下，只有一个动作 $a$ 使得概率 $\pi($ s, $a)$ 取值为 1。对于确定的策略，记为 $a=\pi(s)$

为了对策略函数𝜋进行评估，定义
- 价值函数 (Value Function) 
	$V{:}S\mapsto\mathbb{R}$, 其中 $V_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]$
	即在第 $t$ 步状态为 s 时，按照策略 $\pi$ 行动后在未来所获得反馈值的期望
	$$G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\cdots $$
- 动作-价值函数 (Action-Value Function) 
	$q{:}S\times A\mapsto\mathbb{R}$, 其中 $q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]$
	表示在第 $t$ 步状态为 s 时，按照策略 $\pi$ 采取动作 $a$ 后，在未来所获得反馈值的期望

这样，策略学习转换为如下优化问题：**寻找一个最优策略 $\pi^*$, 对任意 $s\in S$ 使得 $V_\pi^*(s)$ 值最大**


> [!tip] 价值函数与动作-价值函数的关系——贝尔曼方程（Bellman Equation）
> $$V_\pi(s)=\sum_{a\in A}\pi(s,a)q_\pi(s,a)$$
> 
> $$q_\pi(s,a)=\sum_{s^{\prime}\in S}Pr(s^{\prime}|s,a)\left[R(s,a,s^{\prime})+\gamma V_\pi(s^{\prime})\right]$$
> 


---
## 02 策略优化与策略评估

强化学习求解：在策略优化和策略评估的交替迭代中优化参数
![Pasted image 20250529205513.png|475](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250529205513.png)

强化学习的求解方法：
- 基于价值 (Value-based) 的方法
  - 对价值函数进行建模和估计, 以此为依据制订策略
- 基于策略 (Policy-based) 的方法
  - 对策略函数直接进行建模和估计, 优化策略函数使反馈最大化
- 基于模型 (Model-based) 的方法
  - 对环境的运作机制建模,然后进行规划 (planning) 等


### 基于价值（Value-based）的方法

#### 策略优化

给定当前策略 $\pi$、价值函数 $V_\pi$ 和行动-价值函数 $q_\pi$ 时，可如下构造新的策略 $\pi^{\prime}$, $\pi^{\prime}$ 要满足如下条件：
$$\pi ^{\prime }( s) = \arccos _aq_\pi ( s, a)(对于任意s\in S)$$

> [!example]-
> ![Pasted image 20250529210225.png](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250529210225.png)

#### 策略评估

通过迭代计算贝尔曼方程进行策略评估
- 动态规划
- 蒙特卡洛采样
- 时序差分（Temporal Difference）

##### 动态规划

> [!NOTE] 算法流程
> - 初始化 $V_{\pi}$ 函数
> - 循环
> 	- 枚举 $s\in S$
> 	$$\nu_\pi(s) \gets \sum_{a \in A} \pi(s, a) \sum_{s' \in S} Pr(s' | s, a) [R(s, a, s') + \gamma \nu_\pi(s')]$$
> - 直到 $V_{\pi}$ 收敛


> [!example]-
> ![Pasted image 20250529210850.png|450](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250529210850.png)

##### 蒙特卡洛采样

> [!NOTE] 算法流程
> - 选择不同的起始状态，按照当前策略 $\pi$ 采样若干轨迹，记它们的集合为D
> - 枚举 $s\in S$
> 	- 计算 $D$ 中 s 每次出现时对应的反馈 $G_1,G_2,\cdots,G_k$
> 		$$V_{\pi}(s)\leftarrow\frac{1}{k}\sum_{i=1}^{k}G_{i}$$
> 

> [!example]-
> ![Pasted image 20250529211034.png|475](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250529211034.png)

##### 时序差分

> [!NOTE] 算法流程
> - 初始化 $V_\mathrm{\pi}$ 函数
> - 循环
> 	- 初始化 s 为初始状态
> 	- 循环
> 		- $a\sim\pi(s,\cdot)$
> 		- 执行动作 $a$, 观察奖励 $R$ 和下一个状态 $s^\prime$
> 		- 更新 $V_{\pi}(s)\leftarrow V_{\pi}(s)+\alpha[R(s,a,s^{\prime})+\gamma V_{\pi}(s^{\prime})-V_{\pi}(s)]$
> 		- $s\leftarrow s^{\prime}$
> 	- 直到 s 是终止状态
> - 直到 $V_\mathrm{\pi}$ 收敛

> [!example]-
> ![Pasted image 20250529211552.png|475](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250529211552.png)

### 基于策略（Policy-based）的方法

策略函数的参数化可以表示为 $\pi_\theta(s,a)$, 其中 $\theta$ 为一组参数，函数取值表示在状态 $s$ 下选择动作 $a$ 的概率
- 选择一个动作的概率是随着参数的改变而光滑变化的
- 光滑性对算法收敛有更好的保证

最大化目标：$MAXJ(\boldsymbol{\theta}):=V_{\pi_{\boldsymbol{\theta}}}(s_0)$

策略梯度定理 (Policy Gradient Theorem) :

$$\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})\propto\sum_s\mu_{\pi_{\boldsymbol{\theta}}}(s)\sum_aq_{\pi_{\boldsymbol{\theta}}}(s,a)\nabla_{\boldsymbol{\theta}}\pi_{\boldsymbol{\theta}}(s,a)$$

- 其中 $\mu_{\pi_\theta}($ s) 称为策略 $\pi_{\boldsymbol{\theta}}$ 的状态 s 策略分布，假设 $\gamma=1$ (即前面定义的折扣系数)。
	- 在持续问题中，$\mu_{\pi_\theta}(s)$ 为算法从 $s_0$ 出发经过无限多步后位于状态 s 的概率
	- 在分段问题中，$\mu_{\pi_{\theta}}(s)$ 为归一化后的算法从 $s_0$ 出发访问 $s$ 的次数的期望
	- 当 $\gamma\in(0,1)$ 时，则需要给每个状态的 $\mu_{\pi_\theta}$ (s) 值加上一个权重。在保证算法通用性的前提下，为了简化说明，下文在进行公式推导时始终假设 $\gamma=1$, 但是在算法流程中会体现折扣系数 $\gamma$ 的影响。
- 策略梯度法的求解思路：如果能够计算或估计出上述公式中的梯度、智能体就能直接对策略函数进行优化

#TODO 


---
## 03 Q-Learning

> [!NOTE] Q-Learning 算法流程
> - 初始化 $q_\mathrm{\pi}$ 函数
> - 循环
> 	- 初始化 s 为初始状态
> 	- 循环
> 		- $a=\operatorname{argmax}_{a^{\prime}}q_{\pi}(s,a^{\prime})$
> 		- 执行动作 $a$, 观察奖励 $R$ 和下一个状态 $s^\prime$
> 		- 更新 $q_{\pi}(s,a)\leftarrow q_{\pi}(s,a)+\alpha\left[R+\gamma\max_{a^{\prime}}q_{\pi}(s^{\prime},a^{\prime})-q_{\pi}(s,a)\right]$
> 		- $s\leftarrow s^{\prime}$
> 	- 直到 s 是终止状态直到
> - $q_\pi$ 收敛

> [!example]-
> 
> ![Pasted image 20250530134531.png](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530134531.png)
> ![Pasted image 20250530134612.png](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530134612.png)
> ![Pasted image 20250530135029.png](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530135029.png)
> ![Pasted image 20250530135116.png](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530135116.png)
> ![Pasted image 20250530135301.png](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530135301.png)

### 探索（exploration）与利用（exploitation）的平衡

![Pasted image 20250530141237.png|450](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530141237.png)

为何 Q 学习收敛到非最优策略？
- 算法中只有利用没有探索
- (特定的 q 初始 q 函数，会让策略无法改变其轨迹)

大体上利用，偶尔探索👇
$\epsilon$ 贪心（$\epsilon$ -greedy）策略：
$$\epsilon-greedy_{\pi}(s)=\left\{\begin{array}{ll}\operatorname{argmax}_{a} q_{\pi}(s, a), & \text { 以 } 1-\epsilon \text { 的概率 } \\ \text { 随机的 } a \in A, & \text { 以 } \epsilon \text { 的概率 }\end{array}\right.$$

> [!NOTE] 加上 $\epsilon$ 贪心（$\epsilon$ -greedy）策略后的 Q-Learning
> - 初始化$q_\mathrm{\pi}$函数
> - 循环
> 	- 初始化 s 为初始状态
> 	- 循环
> 		- $a = \epsilon-greedy_{\pi}(s)$
> 		- 执行动作$a$, 观察奖励$R$和下一个状态 $s^\prime$
> 		- 更新$q_{\pi}(s, a)\leftarrow q_{\pi}(s, a)+\alpha\left[R+\gamma\max_{a^{\prime}}q_{\pi}(s^{\prime}, a^{\prime})-q_{\pi}(s, a)\right]$
> 		- $s\leftarrow s^{\prime}$
> 	- 直到 s 是终止状态直到
> - $q_\pi$收敛

> [!example]-
> ![Pasted image 20250530141250.png|475](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530141250.png)


---
## 04 深度强化学习

### 深度 Q 学习

- 状态数量太多时，有些状态可能始终无法采样到
- 状态数量无限时，不可能用一张表 (数组) 来记录𝑞函数的值
👉 将𝑞函数参数化（parametrize），用一个非线性回归模型来拟合𝑞函数，例如 (深度) 神经网络

![Pasted image 20250530141516.png|475](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530141516.png)

伪代码：深度 Q 学习
- 初始化$q_\mathrm{\pi}$函数的参数$\theta$
- 循环
	- 初始化 s 为初始状态
	- 循环
		- 采样$a\sim\epsilon-greedy_{\pi}(s;\theta)$
		- 执行动作$a$, 观察奖励$R$和下一个状态 s'
		- 损失函数 L$(\theta)=\frac12\begin{bmatrix}R+\gamma\max_{a^{\prime}}q_{\pi}(s^{\prime}, a^{\prime};\theta)-q_{\pi}(s, a;\theta)\end{bmatrix}^2$
		- 根据梯度$\partial L (\theta)/\partial\theta$更新参数$\theta$
		- $s\leftarrow s^{\prime}$
	- 直到 s 是终止状态
- 指导 $q_{\pi}$ 收敛

两个不稳定因素：
- 相邻的样本来自同一条轨迹，样本之间相关性太强，集中优化相关性强的样本可能导致神经网络在其他样本上效果下降
- 在损失函数中，𝑞函数的值既用来估计目标值，又用来计算当前值。现在这两处的𝑞函数通过𝜃有所关联，可能导致优化时不稳定

经验重现 Experience Replay
- 将过去的经验存储下来，每次将新的样本加入到存储中去，并从存储中采样一批样本进行优化 
- 解决了样本相关性强的问题 
- 重用经验，提高了信息利用的效率
![Pasted image 20250530142116.png|425](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530142116.png)

目标网络 Target Network
	$$\frac12[R+\gamma\max_{a^{\prime}}\boxed{q_\pi (s^{\prime}, a^{\prime};\theta^{-})}-q_\pi (s, a;\theta)]^2$$
- 损失函数的两个𝑞函数使用不同的参数计算
- 用于计算估计值的$q$使用参数$\theta^-$计算，这个网络叫做目标网络
- 用于计算当前值的$q$使用参数$\theta$计算
- 保持$\theta^-$的值相对稳定，例如$\theta$每更新多次后才同步两者的值
$$\theta^{-}\leftarrow\theta $$

![Pasted image 20250530142322.png|450](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530142322.png)


### Tips of Q-Learning

> [!tip] 一些科研的经验分享


---
## 05 多智能体强化学习

![Pasted image 20250530143756.png|500](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530143756.png)

两个问题：
- 多智能体信用分配问题 (Credit Assignment)
	- 当只存在 Global reward 时，如何为每个智能体分配 Reward or Value function
- 多智能体通信学习问题 (Communication Learning)
	- 每个智能体如何与其他智能体进行高效的交流

> [!tip] 沙普利值
> 
> ![Pasted image 20250530144043.png|450](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530144043.png)
> ![Pasted image 20250530144058.png|450](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530144058.png)
> ![Pasted image 20250530144112.png|450](/img/user/CS3022F%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530144112.png)

