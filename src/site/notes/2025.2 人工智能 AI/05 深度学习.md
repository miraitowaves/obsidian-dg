---
{"dg-publish":true,"permalink":"/2025.2 人工智能 AI/05 深度学习/","dgPassFrontmatter":true,"noteIcon":""}
---

## 01 前馈神经网络

### 基本概念

神经元是深度学习模型中基本单位。可以如下刻画神经元功能：
1. 对相邻前向神经元输入信息进行加权累加：$In=\sum_i^n=w_i*a_i$
2. 对累加结果进行非线性变换 (通过激活函数) $:g(x)$
3. 神经元的输出：Out $=g(In)$

![Pasted image 20250422095711.png|375](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422095711.png)

神经网络使用非线性函数作为激活函数（activation function），通过对多个非线性函数进行组合，来实现对输入信息的非线性变换：
![Pasted image 20250422095734.png|375](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422095734.png)

Softmax 函数一般用于多分类问题中，其将输入数据 $x_i$ 映射到第 $i$ 个类别的概率 $y_i$ 如下计算：
$$y_i=\mathrm{softmax}(x_i)=\frac{e^{x_i}}{\sum_{j=1}^ke^{x_j}}$$
![Pasted image 20250422095830.png|250](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422095830.png)

前馈神经网络（feedforward neural network）
- 各个神经元接受前一级的输入，并输出到下一级，模型中没有反馈
- 层与层之间通过“全连接”进行链接，即两个相邻层之间的神经元完全成对连接，但层内的神经元不相互连接

损失函数（Loss Function）：计算模型预测值与真实值之间的误差
- 均方误差损失函数：计算预测值和实际值之间距离（即误差）的平方来衡量模型优劣
	$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$
- 交叉熵损失函数：度量两个概率分布间的差异
	$$H(p,q)=-\sum_{x}p(x)*logq(x)$$
	交叉熵越小，两个概率分布 p 和 q 越接近
	Softmax 和交叉熵损失函数相互结合，为偏导计算带来了极大便利 Softmax with cross-entropy loss

### 感知机模型

早期的感知机结构和 MCP 模型相似，由一个输入层和一个输出层构成，因此也被称为 “单层感知机”。感知机的输入层负责接收实数值的输入向量，输出层则能输出 1 或 -1 两个值
![Pasted image 20250530144957.png|400](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530144957.png)

多层感知机由输入层、输出层和至少一层的隐藏层构成
- 网络中各个隐藏层中神经元可接收相邻前序隐藏层中所有神经元传递而来的信息，经过加工处理后将信息输出给相邻后续隐藏层中所有神经元。
- 各个神经元接受前一级的输入，并输出到下一级，模型中没有反馈 
- 层与层之间通过“全连接”进行链接，即两个相邻层之间的神经元完全成对连接，但层内的神经元不相互连接
- 也就是前馈神经网络 feedforward neural network
![Pasted image 20250530145120.png|475](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530145120.png)

### 如何优化网络参数？

从标注数据出发，优化模型参数
- 标注数据：$(x_i,y_i)(1\leq i\leq N)$
- 评分函数 (scoring function) 将输入数据映射为类别置信度大|小：s $=f(x)=W\varphi(x)$
- 损失函数来估量模型预测值与真实值之间的差距。损失函数给出的差距越小，则模型鲁棒性就越好。|常用的损失函数有 softmax 或者 SVM
![Pasted image 20250530150821.png|425](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530150821.png)

#### 梯度下降 Gradient Descent

梯度下降算法是一种使得损失函数最小化的方法。一元变量所构成函数𝒇在𝒙处梯度为：
$$\frac{df(x)}{dx} = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}$$
- 在多元函数中，梯度是对每一变量所求导数组成的向量
	$$\nabla f(\mathbf{x})=\begin{bmatrix}\frac{\partial f(\mathbf{x})}{\partial x_1}\\\frac{\partial f(\mathbf{x})}{\partial x_2}\\\vdots\\\frac{\partial f(\mathbf{x})}{\partial x_n}\end{bmatrix}$$
- 梯度的反方向是函数值下降最快的方向，因此是损失函数求解的方向

假设损失函数 $f(x)$ 是连续可微的多元变量函数, 其泰勒展开如下 ( $\Delta x$ 是微小的增量):
$$f(x+\Delta x)=f(x)+f'(x)\Delta x+\frac{1}{2}f''(x)(\Delta x)^2+\cdots+\frac{1}{n!}f^{(n)}(x) (\Delta x)^n$$
$$
f(x+\Delta x)-f(x)\approx(\nabla f(x))^T\Delta x$$
- 为了保证 $(\nabla f(\mathbf{x}))^T\Delta\mathbf{x}<0$, 我们选择让 $\Delta\mathbf{x}$ 与梯度 $\nabla f(\mathbf{x})$ 的方向相反。最简单的选择是让
$\Delta\mathbf{x}$ 正比于梯度的负方向：
	$$\Delta\mathbf{x}=-\eta\nabla f(\mathbf{x})$$
- 其中，$\eta$ 是一个正的常数，称为学习率 (learning rate) 或步长 (step size)。学习率控制着每次迭代中参数更新的幅度。
- 将 $\Delta\mathbf{x}$ 代入 $\mathbf{x}+\Delta\mathbf{x}$, 得到参数的更新公式：
	$$\mathbf{x}_{new}=\mathbf{x}_{old}-\eta\nabla f(\mathbf{x}_{old})$$
- 这个公式表示在每次迭代中，我们都沿着当前点 $\mathbf{x}_{old}$ 处梯度 $\nabla f(\mathbf{x}_{old})$ 的反方向移动一小步 (步长为 $\eta$), 从而更新参数到 $\mathbf{x}_{new}$, 希望能够逐步接近损失函数的最小值

#### 误差反向传播 Error Back Propagation (BP)

BP 算法是一种将输出层误差反向传播给隐藏层进行参数更新的方法

![Pasted image 20250530152625.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530152625.png)

> [!note]- 前向传播
> 
> ![Pasted image 20250530152657.png|425](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530152657.png)
> ![Pasted image 20250530152702.png|425](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530152702.png)
> ![Pasted image 20250530152710.png|425](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530152710.png)
> ![Pasted image 20250530152716.png|425](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530152716.png)
> ![Pasted image 20250530152723.png|425](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530152723.png)

> [!note]- 反向传播-梯度下降
> 
> ![Pasted image 20250530152808.png|425](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530152808.png)
> ![Pasted image 20250530152823.png|425](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530152823.png)
> ![Pasted image 20250530152847.png|425](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530152847.png)

#### 梯度下降算法

- 批量梯度下降（Batch Gradient Descent, BGD） 
	- 一次迭代对所有样本进行计算，利用矩阵进行操作可实现并行
	- 当目标函数为凸函数时，BGD 一定能够得到全局最优
	- 当样本数目很大时，每迭代一步都需要对所有样本计算，训练过程会很慢
- 随机梯度下降（Stochastic Gradient Descent，SGD） 
	- 在每轮迭代中，随机优化一个样本的损失，每一轮参数的更新速度大大加快
	- 准确度下降。即使目标函数为强凸函数，SGD 仍无法线性收敛
	- 可能会收敛到局部最优，由于单个样本不能代表全体样本趋势
	- 不易于实现并行
- 小批量梯度下降（Mini-Batch Gradient Descent, MBGD）
	- 每次迭代使用 **batch_size** 个样本来对参数进行更新

> [!NOTE] batch size 的选择带来的影响
> 
> 在合理地范围内，增大 batch size 的好处：
> - 内存利用率提高了
> - 跑完一次 epoch (全数据集) 所需的迭代次数减少
> - 在一定范围内，一般来说 Batch\_Size 越大，其确定的下降方向越准，引起训练震荡越小
> 
> 盲目增大 batch\_size 的坏处：
> - 内存容量可能撑不住了
> - 跑完一次 epoch (全数据集) 所需的迭代次数减少，但要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢
> - Batch Size 增大到一定程度，其确定的下降方向已经基本不再变化


---
## 02 卷积神经网络 CNN

为什么需要卷积神经网络👇
当模型参数数量变得巨大时，不仅会占用大量计算机内存，同时也使神经网络模型变得难以训练收敛。因此，对于图像这样的数据，不能直接将所构成的像素点向量与前馈神经网络神经元相连。

### 2.1 卷积

- 卷积（convolution）：就是针对像素点的空间依赖性来对图像进行处理的一种技术
- 卷积核（kernel）：一个二维矩阵 (可以是多维的，例如 RGB 图像，有三维的卷积核，分别针对三个通道进行 convolution)
	- 权重：通过数据驱动机制学习得到，捕捉图像空间特征
- 卷积操作：
	- 步长 Stride：每进行一次卷积计算后，卷积核移动的格数
- 感受野（receptive field）：感受野是特征图上一个点对应输入图像上的区域


不同卷积核可被用来刻画视觉神经细胞对外界信息感受时的不同选择性：
![Pasted image 20250422103311.png|475](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422103311.png)


### 2.2 池化

图像中存在较多冗余，可用某一区域子块的统计信息 （如最大值或均值等）来刻画该区域中所有像素点呈现的空间分布模式

池化 Pooling: 对输入的特征图进行下采样，以获得最主要的信息
- Max Pooling
- Mean Pooling
- Stochastic Pooling (对像素点按照数值大小赋予概率)
![Pasted image 20250422103948.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422103948.png)



分布式向量表达 (distributed vector representation)：卷积神经网络在若干次卷积操作、接着对卷积所得结果进行激活函数操作和池化操作下，最后通过全连接层来学习得到输入数据的特征表达
- 卷积层：提取局部特征
- 池化层：降维
- 激活函数：非线性化
- 全连接层：用于输出想要的结果 (下游任务)
![Pasted image 20250422104649.png|425](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422104649.png)

### 2.3 正则化

正则化系数 👉 缓解神经网络在训练过程中出现的过拟合问题

有如下几种方法：
- Dropout
	每次参数更新时随机丢掉一部分神经元来减少神经网络复杂度，防止过拟合
- Batch-Normalization（批归一化）
	通过规范化的手段，把神经网络每层中任意神经元的输入值分布改变到均值为 0、方差为 1 的标准正态分布
- L1-Norm & L2-Norm
	$L_1$ 范数：数学表示为 $\|W\|_1=\Sigma_{i=1}^\mathbb{N}|w_i|$, 指模型参数 $W$ 中各个元素的绝对值之和
	$L_1$ 范数也被称为 “稀疏规则算子” ( Lasso regularization)  
	$L_2$ 范数：数学表示为 $\|W\|_2=\sqrt{\Sigma_{i=1}^\mathbb{N}w_i^2}$, 指模型参数 $W$ 中各个元素平方和的开方
	$Loss=原始损失+\lambda×正则项$
	"结构风险最小化"

### 2.4 AlexNet

![Pasted image 20250422111145.png|400](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422111145.png)

- 当去掉的层数过多时，性能会发生明显的下降
![Pasted image 20250422111116.png|400](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422111116.png)


---
## 03 循环神经网络 RNN

循环神经网络：一类处理序列数据（如文本句子、视频帧等）时所采用的网络结构
- 本质是希望模拟人所具有的记忆能力，在学习过程中记住部分已经出现的信息，并利用所记住的信息影响后续结点输出

### 3.1 基本架构

在时刻 $t$, 一旦得到当前输入数据 $x_t$, 循环神经网络会结合前一时刻 $t-1$ 得到的隐式编码 $h_{t-1}$, 如下产生当前时刻隐式编码 $h_t:$
	$$h_t=\Phi(\mathrm{U}\times x_t+\mathrm{W}\times h_{t-1})$$
- 这里 $\Phi(\cdot)$ 是激活函数，一般可为 Sigmoid 或者 Tanh 激活函数，使模型能够忘掉无关的信息，同时更新记忆内容。U 与 W 为模型参数

![Pasted image 20250422112301.png|400](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422112301.png)

循环神经网络读入每个单词后，先产生对应单词的隐式编码，再如下得到一个句子的向量编码：
1. 最后一个单词输出作为整个句子的编码
	- 因为句子中所有单词的信息均被后向传递
2. 将每个单词的隐式编码进行加权平均，将加权平均结果作为整个句子的向量编码表示
![Pasted image 20250607204644.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250607204644.png)


### 3.2 梯度传递

- 时刻 $t$ 输入数据 $x_t$ (文本中的单词) 的隐式编码为 $h_t$、其真实输出为 $y_t$ (单词词性)、模型预测 $x_t$ 的词性是 $O_t$
- 参数 $W_x$ 将 $x_t$ 映射为隐式编码 $h_t$、参数 $W_o$ 将 $h_t$ 映射为预测输出 $O_t$、$h_{t-1}$ 通过参数 $W_h$ 参与 $h_t$ 的生成。
- 图中 $W_x$、$W_o$ 和 $W_h$ 是复用参数
![Pasted image 20250422113534.png](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422113534.png)


### 3.3 基本结构

一些基本的 RNN 结构：
- 多对多 👉 机器翻译
- 多对一 👉 情感分类
- 一对多 👉 图像描述生成
![Pasted image 20250422114342.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422114342.png)

双向循环神经网络：
e.g. 我的手机坏了，我打算\_\_\_\_一部新手机

![Pasted image 20250422114741.png|400](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422114741.png)

堆叠（多层）循环神经网络：
![Pasted image 20250607205152.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250607205152.png)




### 3.4 LSTM

由于 tanh 函数的导数取值位于 0 到 1 区间，对于长序列而言，若干多个 0 到 1 区间的小数相乘，会使得参数求导结果很小，引发梯度消失问题。为了解决梯度消失问题，长短时记忆模型（LongShort-Term Memory，LSTM）被提出


![Pasted image 20250422115238.png|475](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422115238.png)

| 符号      | 内容描述/操作                                                                                                                                                                                              |
| ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| $x_t$   | 时刻 t 输入的数据                                                                                                                                                                                           |
| $i_t$   | 输入门的输出: $i_t = sigmoid (W_{xi}x_t + W_{hi}h_{t-1} + b_i)$<br>( $W_{xi}$ 、 $W_{hi}$ 和 $b_i$ 为输入门的参数)<br>**输入门 $i_t$ 控制有多少信息流入当前时刻内部记忆单元 $c_t$**                                                       |
| $f_t$   | 遗忘门的输出: $f_t = sigmoid (W_{xf}x_t + W_{hf}h_{t-1} + b_f)$<br>( $W_{xf}$ 、 $W_{hf}$ 和 $b_f$ 为遗忘门的参数)<br>**遗忘门控制上一时刻内部记忆单元 $c_{t-1}$ 中有多少信息可累积到当前时刻内部记忆单元 $c_t$**                                      |
| $o_t$   | 输出门的输出: $o_t = sigmoid (W_{xo}x_t + W_{ho}h_{t-1} + b_o)$<br>( $W_{xo}$ 、 $W_{ho}$ 和 $b_o$ 为输出门的参数)<br>**输出门控制 $c_t$ 最终向 $h_t$ 的输出情况**                                                               |
| $c_t$   | 内部记忆单元的输出：$c_t={f_t}\odot c_{t-1}+{i_t}\odot tanh (W_{xc}{x_t}+W_{hc}{h_{t-1}}+b_c)$<br>$(W_{xc}$、$W_{hc}$ 和 $b_c$ 为记忆单元的参数）                                                                         |
| $h_t$   | 时刻 $t$ 输入数据的隐式编码：<br>$$h_{t}=o_{t}\odot\tanh (c_{t})=o_{t}\odot\tanh (f_{t}\odot c_{t-1}+i_{t}\odot\tanh (W_{xc}x_{t}+W_{hc}h_{t-1}+b_{c}))$$<br>**(输入门、遗忘门和输出门的信息 $i_t$、$f_t$、$o_t$ 一起参与得到 $h_t$）** |
| $\odot$ | 两个向量中对应元素按位相乘 (element-wise product)<br>如: $[10\ 6\ 3\ 7]\odot[0.2\ 0.1\ 0.8\ 0.5]=[2\ 0.6\ 2.4\ 3.5]$                                                                                               |

一些细节：
- 三种门结构的输出 $i_t$、$f_t$ 和 $o_t$ 值域为 $(0,1)$
- 每个时刻 t 只有 $ht$ 作为本时刻的输出以用于分类等处理
	- 实际上，在每个时刻 $t$ 中只有内部记忆单元信息 $C_t$ 和隐式编码 $h_t$ 这两种信息起到了对序列信息进行传递的作用
- LSTM 如何克服梯度消失？
	- LSTM 通过引入门结构，在从 t 到 t+1 过程中引入加法来进行信息更新，避免了梯度消失问题

#### GRU

门控循环单元（gated recurrent unit，GRU）神经网络

对 LSTM 作了一定的简化：不用记忆单元，仅使用隐藏状态来进行信息的传递
![Pasted image 20250422120852.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422120852.png)



### 3.5 注意力机制


> [!tip] 
> 可以看看这篇文章：[Transformer模型详解（图解最完整版） - 知乎](https://zhuanlan.zhihu.com/p/338817680)

首先生成每个单词的内嵌向量 (包含了单词在句子中位置编码向量信息), 记为 $w_i (1\leq i\leq 4)$, 如下计算每个单词 $w_i$ 的查询向量 (query)、键向量 (key) 和值向量 (value):
- 查询向量：$q_i=W^q\times w_i$ ；搜索单词
- 键向量：$k_i=W^k\times w_i$ ；被搜索物品的描述信息
- 值向量：$v_{i}=W^{v}\times w_{i}$ ；被搜索物品的语义内容

后面可以看到，对每个单词而言 $w_i, w^q$、$w^k$ 和 $w^v$ 三个映射矩阵都是一样的，也是自注意力模型需要训练的全部参数

自注意力模型就是要挖掘单词 $w_i$ 与其他单词在句子中因为上下文 (context) 关联而具有的自注意力取值大小。以图 5.21 中单词 $w_3$ 与其他单词之间自注意力取值大小计算为例：
1. 计算 $w_{3}$ 所对应查询向量与其他单词健向量之间的点积，该点积可作为单词 $w_{3}$ 与其他单词之间的关联度：$\alpha_{3 i}=q_{3}\cdot k_{i}$
2. 对 $\alpha_{3 i}$ 结果通过 softmax 函数进行归一化操作，得到 $a_{3 i}^{\prime}$
	对于给定“项庄舞剑、意在沛公”句子，$a_{3 i}^{\prime}$ 记录了“意在”单词与其他单词所具有的重要程度 (即注意力)
3. $a_{3 i}^{\prime}$ 乘以每个单词 $w_i$ 所对应的值向量 $v_i$, 即 $a_{3 i}^{\prime}\times v_i.$
4. 对上一步结果进行 $\Sigma_ia_{3 i}^{\prime}\times v_i$, 这个结果作为在当前句子语境下单词 $w_{2}$ “注意”到与其他单词的关联程度 (self-attention)


![Pasted image 20250422121437.png|475](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250422121437.png)

Multi-Head Attention: 构造不同的 Self-Attention 识别不同的特征

---
## 04 深度生成学习

生成模型需要学习目标数据的分布规律，以合成属于目标数据空间的新数据：
![Pasted image 20250530154454.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530154454.png)

一些生成模型：
- 自编码器 Autoencoder AE
	AE 的 Decoder 只能保证将由 x 生成的 code 还原为 x
	如果我们随机生成 1 个 code，经过 AE 的 Decoder 后往往不会得到有效的图像
	![Pasted image 20250530154652.png|300](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530154652.png)
- 变分自编码器 Variational Autoencoder VAE
	![Pasted image 20250530154719.png|300](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530154719.png)

### 生成对抗网络  GAN

生成对抗网络由一个生成器（generator，简称 G）和一个判别器（discriminator，简称 D）组成

GAN 的核心是通过生成器和判别器两个神经网络之间的竞争对抗，不断提升彼此水平以使得生成器所生成数据（人为伪造数据）与真实数据相似，判别器无法区分真实数据和生成数据

![Pasted image 20250530154747.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530154747.png)

目标函数为：
$$\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)}[\log(1 - D(G(z)))]$$

> [!NOTE] 训练 GAN
> - 输入：神经网络 D、G，噪声分布 pz (z)，真实数据分布 pdata (x)
> - 输出：神经网络参数θd、θg
> ---
> - 算法步骤：
> 	- 每轮训练循环执行：
> 	- 训练 k 轮判别器：
> 		- 从噪声分布 $p_z (z)$ 中采样 $m$ 个样本 $\{z^(1), z^(2), z^{(3)}\ldots z^{(m)}\}$
> 		- 从真实数据分布 $p_{data}(x)$ 中采样 $m$ 个样本 $\{x^{(1)}, x^{(2)}, x^{(3)}\ldots x^{(m)}\}$
> 		- 沿梯度上升方向更新判别器参数：
> 			$\nabla_{\theta_{d}}\frac{1}{\mathrm{m}}\sum_{i=1}^{m}[\log D\bigl (x^{(i)}\bigr)+\log\left (1-D\left (G\bigl (z^{(i)}\bigr)\right)\right)]$
> 	- 从噪声分布 $p_z (z)$ 中采样 $m$ 个样本 $\{Z^{(1)}, Z^{(2)}, Z^{(3)}... Z^{(m)}\}$
> 	- 沿梯度下降方向更新判别器参数：
> 		$$\nabla_{\theta_{g}}\frac{1}{\mathrm{m}}\sum_{i=1}^{m}\log\left (1-D\left (G\big (z^{(i)}\big)\right)\right)$$
> 

GAN 具有收敛困难，且非常依赖于合适的交替训练轮次选择等不足，于是出现了 👇

Wasserstein GAN
- 模型输出不使用以概率值形式输出的 sigmoid 激活函数，而是直接根据输出值大小评价其效果
- 相应地，目标函数中也不需要再取对数：
	$$\min_G\max_D V (D, G) = \mathbb{E}_{x\sim p_{data}(x)}[D (x)] - \mathbb{E}_{z\sim p_z (z)}[D (G (z))]$$

> [!tip] 条件生成网络
> 多了一个约束条件 y，输入到生成网络 G 和判别网络 D 中：
> ![Pasted image 20250530155245.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250530155245.png)


### 对抗样本攻击

加入随机噪声后，尽管人眼仍能明显地识别图像的主体，但神经网络会因此产生明显的判断错误：
![Pasted image 20250607211128.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250607211128.png)

对抗生成训练器模型
- 让生成器扮演攻击者：高效地生成带有扰动的对抗样本，帮助分类网络学到"扰动"
- 让判别器扮演分类器
![Pasted image 20250607211244.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250607211244.png)


---
## 05 深度学习应用

“词袋”（Bag of Words）模型：一个单词按照词典序被表示为一个词典维数大小的向量（被称为 one hot vector），向量中该单词所对应位置按照其在文档中出现与否取值为 1 或 0。1 表示该单词在文档中出现、0 表示没有出现
- 无法有效计算单词与单词之间的语义相似度

分布式向量表达 (distributed vector representation) 👉 利用深度学习模型，可将每个单词表征为𝑁维实数值的分布式向量

### Word2Vec

如何获得单个单词的词向量？
1. 将该单词表示成 $V$ 维 one-hot 向量 $X$
2. 隐藏层神经元大小为 $N$, 每个神经元记为 $h_i (1\leq i\leq N)$
3. 向量 $X$ 中每个 $x_i (1\leq i\leq$ $V)$ 与隐藏层神经元是全连接，连接权重矩阵为 $W_{V\times N}$
4. 这里 $W_{V\times N}$ 和 $W_{N\times V}^{\prime}$ 为模型参数
5. 一旦训练得到了下图的神经网络，就可以将输入层中 $x_k$ 与隐藏层连接权重为 $\{w_{k1}, w_{k2},\cdots, w_{kN}\}$ (隐藏层) 作为第 $k$ 个单词 $N$ 维词向量 (word vector)

![Pasted image 20250607211951.png|450](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250607211951.png)

> [!tip] Word2Vec 的两种训练模式
> Continuous Bag-of-Words (CBoW)：根据某个单词所处的上下文单词来预测该单词
> 
> 可用 $x_k$ 的前序 $k-1$ 个单词和后续单词 $c-k$ 个单词一起来预测 $x_k$
> - 每个输入单词仍然是 $V$ 维向量（可以是 one-hot 的表达）
> - 隐式编码为每个输入单词所对应编码的均值
> 
> ![Pasted image 20250607212711.png|400](/img/user/2025.2%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%20AI/public/Pasted%20image%2020250607212711.png)
> 
> Skip-gram：利用某个单词来分别预测该单词的上下文单词
> 
> 无论是 CBOW 还是 Skip-Gram 模式，随着单词个数数目增加，模型参数数目也迅速上升
> 👉 加快模型训练：层次化 Softmax（Hierarchical Softmax）与负采样（Negative Sampling）


